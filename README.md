# microopt
CS389 HW The Last

## Naive Implementation
I did a pretty basic conversion loop using the trick of `int digit = char - '0'` to convert the string in question to an integer. This ran for me at 0.006830s in the fastest of five trials, in comparison to the basic `atoi()` which was only as fast as 0.035011s. This means my naive implementation by comparison is not ten times faster than `atoi()` as the moodle suggests it should be; however, I see now obvious issues that could be slowing it down (except for the ones that I put into my optimization).
As far as the reason for this improvement, the only thing I can think of is that my code gets to make a few more assumptions: that the string contains only digits, with no decimals, negative signs, or other characters. Other than that, it seems like it ought to function just as `atoi()` does, so I guess that `atoi()` is just slowed down by checking for those and other non-convertible characters.

## Optimization
I admittedly have done this project once before, as an extra credit assignment for CS 221, so I followed the advice from then of inspecting the source file. It appears that all strings were between 3 and 5 digits long, so I replaced my loop with three explicit instances of conversion, then a pair of nested if loops to check if the string was 4 or 5 digits long and perform the extra conversions as necessary. I also delayed the data access as much as possible, instead working with an integer sum and loading that into the array at the end. I made a similar change for the subtraction, now using `int digit = char` and subtracting the ascii value of zero times the appropriate factor at the end. To do this, I subtracted `111*'0'` for the first three digits, and added an extra `1000*'0'` for numbers that were four digits long, and likewise `10000*'0''` for those that reached five digits.
However, I could not find any optimizations for the multiplying by ten part of the conversion. It doesn't seem possible to delay this like the subtraction is delayed, since the digits need to be multiplied by different values depending on what place they were in. Changing the multiplication to work on the digit directly also did not work, and neither did replacing the multiplication with a series of logical shifts. (`10*sum` being equivalent to `(sum << 3) + (sum << 1)`, a trick that seemed to improve performance for me last time.) I fiddled with a few other things but couldn't seem to find any further optimization. This resulted in a best time performance of 0.004153s out of 5 trials of 1000, which gives me about 39% improvement from my naive implementation.
For further optimizations, I might consider trying to handle my variables differently, finding a more efficient solution than `if(digit = '\0')`, or perhaps making some optimization to the basic loop. However, with the exception of removing the loop and replacing it with the code written 1000 times over with increasing values of `i`, I don't know how I might make any of these improvements.
